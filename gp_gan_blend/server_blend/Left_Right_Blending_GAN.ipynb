{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Left Right Blending GAN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hHO4RbdTajOO",
        "outputId": "b687ee4f-83ec-48e3-8f74-8b07de7b8ec4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"]=\"/content/drive/My Drive/Colab Notebooks/nth-honor-339920-5256e664e9e6.json\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "  DOCKERFILE\n",
        "'''\n",
        "# FROM pytorch/pytorch:latest\n",
        "# WORKDIR /app\n",
        "# COPY requirements.txt requirements.txt\n",
        "# RUN pip install -r requirements.txt\n",
        "# RUN mkdir tocrop cropped models\n",
        "# COPY mask_display.png tocrop/\n",
        "# COPY unsupervised_blending_gan.npz models/ \n",
        "# COPY *.py .\n",
        "# EXPOSE 8080\n",
        "# CMD [ \"python3\", \"main.py\" ]\n",
        "\n",
        "\n",
        "\n",
        "# Folder Structure\n",
        "# app -> tocrop, cropped, models -> tocrop:mask_display.png , models:usupervised..."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "1wXmbIbksmsK",
        "outputId": "d3305f41-70c5-413c-cbfb-231dd00c2beb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n  DOCKERFILE\\n'"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "  Reference: main.py routing calls\n",
        "  Summons perform_blend.py's Merger class, sends src and dst and blended-result-name\n",
        "'''\n",
        "\n",
        "# blender = Merger()\n",
        "\n",
        "# @app.route(\"/blend\", methods=[\"GET\"])\n",
        "# def get_result_url():\n",
        "#     # source = request.args.to_dict()['image_src']\n",
        "#     # destin = request.args.to_dict()[\"image_dst\"]\n",
        "#     # blended = request.args.to_dict()[\"blend_id\"]\n",
        "    \n",
        "#     # result_BLOB = blender.get_res_blob(\n",
        "#     #     source, destin, blended\n",
        "#     # )\n",
        "\n",
        "#     # return jsonify({\"result_BLOB\": result_BLOB})\n",
        "#     data_dict = request.args.to_dict()[\"instances\"]\n",
        "\n",
        "#     # image source # image dest # blend id\n",
        "#     logging.info(data_dict)\n",
        "\n",
        "\n",
        "\n",
        "#     source = data_dict[0][\"image_src\"]\n",
        "#     destin = data_dict[0][\"image_dst\"]\n",
        "#     blended = data_dict[0][\"blend_id\"]\n",
        "\n",
        "#     result_BLOB = blender.get_res_blob(source, destin, blended)\n",
        "\n",
        "#     return jsonify({\"result_BLOB\": result_BLOB})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "YRdJUXO3qX_v",
        "outputId": "7f6564d5-fc1f-472b-923d-9a8c1df4fe1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\n  Reference: main.py routing calls\\n  Summons perform_blend.py's Merger class, sends src and dst and blended-result-name\\n\""
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "  Reference: cropper.py\n",
        "'''\n",
        "\n",
        "from skimage.io import imread, imsave\n",
        "import os\n",
        "# tocrop = os.path.join(os.getcwd(), 'tocrop')\n",
        "# cropped = os.path.join(os.getcwd(), 'cropped')\n",
        "tocrop = \"/content/drive/My Drive/Final Project CS 329S/Blending GAN/Blending Sandbox Output/images/left_right_images/tocrop\"\n",
        "cropped = \"/content/drive/My Drive/Final Project CS 329S/Blending GAN/Blending Sandbox Output/images/left_right_images/cropped\"\n",
        "sx,sy,ex,ey = 4,5,476,494\n",
        "\n",
        "def cropper():\n",
        "    for img in os.listdir(tocrop):\n",
        "        #(root,ext) = os.path.splitext(img)\n",
        "        mask = imread(str(os.path.join(tocrop, img)))\n",
        "        cropped_mask = mask[sx:ex, sy:ey]\n",
        "        mask_name = \"crop_\" + str(img)\n",
        "        imsave(os.path.join(cropped, mask_name), cropped_mask)"
      ],
      "metadata": {
        "id": "w4o_JMHkq5HP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install chainer==6.3.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 786
        },
        "id": "PH21PMHHwgEP",
        "outputId": "7e7c0278-bdb7-4373-ef47-1ea7622a7b92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting chainer==6.3.0\n",
            "  Downloading chainer-6.3.0.tar.gz (874 kB)\n",
            "\u001b[?25l\r\u001b[K     |▍                               | 10 kB 36.1 MB/s eta 0:00:01\r\u001b[K     |▊                               | 20 kB 41.4 MB/s eta 0:00:01\r\u001b[K     |█▏                              | 30 kB 25.9 MB/s eta 0:00:01\r\u001b[K     |█▌                              | 40 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |█▉                              | 51 kB 12.7 MB/s eta 0:00:01\r\u001b[K     |██▎                             | 61 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |██▋                             | 71 kB 14.1 MB/s eta 0:00:01\r\u001b[K     |███                             | 81 kB 13.2 MB/s eta 0:00:01\r\u001b[K     |███▍                            | 92 kB 14.6 MB/s eta 0:00:01\r\u001b[K     |███▊                            | 102 kB 13.6 MB/s eta 0:00:01\r\u001b[K     |████▏                           | 112 kB 13.6 MB/s eta 0:00:01\r\u001b[K     |████▌                           | 122 kB 13.6 MB/s eta 0:00:01\r\u001b[K     |████▉                           | 133 kB 13.6 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 143 kB 13.6 MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 153 kB 13.6 MB/s eta 0:00:01\r\u001b[K     |██████                          | 163 kB 13.6 MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 174 kB 13.6 MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 184 kB 13.6 MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 194 kB 13.6 MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 204 kB 13.6 MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 215 kB 13.6 MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 225 kB 13.6 MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 235 kB 13.6 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 245 kB 13.6 MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 256 kB 13.6 MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 266 kB 13.6 MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 276 kB 13.6 MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 286 kB 13.6 MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 296 kB 13.6 MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 307 kB 13.6 MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 317 kB 13.6 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 327 kB 13.6 MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 337 kB 13.6 MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 348 kB 13.6 MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 358 kB 13.6 MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 368 kB 13.6 MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 378 kB 13.6 MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 389 kB 13.6 MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 399 kB 13.6 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 409 kB 13.6 MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 419 kB 13.6 MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 430 kB 13.6 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 440 kB 13.6 MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 450 kB 13.6 MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 460 kB 13.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 471 kB 13.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 481 kB 13.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 491 kB 13.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 501 kB 13.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 512 kB 13.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 522 kB 13.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 532 kB 13.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 542 kB 13.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 552 kB 13.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 563 kB 13.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 573 kB 13.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 583 kB 13.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 593 kB 13.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 604 kB 13.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 614 kB 13.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 624 kB 13.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 634 kB 13.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 645 kB 13.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 655 kB 13.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 665 kB 13.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 675 kB 13.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 686 kB 13.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 696 kB 13.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 706 kB 13.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 716 kB 13.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 727 kB 13.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 737 kB 13.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 747 kB 13.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 757 kB 13.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 768 kB 13.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 778 kB 13.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 788 kB 13.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 798 kB 13.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 808 kB 13.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 819 kB 13.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 829 kB 13.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 839 kB 13.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 849 kB 13.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 860 kB 13.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 870 kB 13.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 874 kB 13.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from chainer==6.3.0) (57.4.0)\n",
            "Collecting typing<=3.6.6\n",
            "  Downloading typing-3.6.6-py3-none-any.whl (25 kB)\n",
            "Collecting typing_extensions<=3.6.6\n",
            "  Downloading typing_extensions-3.6.6-py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from chainer==6.3.0) (3.6.0)\n",
            "Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from chainer==6.3.0) (1.21.5)\n",
            "Collecting protobuf<3.8.0rc1,>=3.0.0\n",
            "  Downloading protobuf-3.7.1-cp37-cp37m-manylinux1_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 64.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from chainer==6.3.0) (1.15.0)\n",
            "Building wheels for collected packages: chainer\n",
            "  Building wheel for chainer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for chainer: filename=chainer-6.3.0-py3-none-any.whl size=883599 sha256=263658bd0c73f22d46db13212e48efb8ce73471c69a963ec6008e9b309e71338\n",
            "  Stored in directory: /root/.cache/pip/wheels/be/67/97/6fa45a07187f9cf6968adc6c57c5af94c975e374534a56fff7\n",
            "Successfully built chainer\n",
            "Installing collected packages: typing-extensions, typing, protobuf, chainer\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing-extensions 3.10.0.2\n",
            "    Uninstalling typing-extensions-3.10.0.2:\n",
            "      Successfully uninstalled typing-extensions-3.10.0.2\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.17.3\n",
            "    Uninstalling protobuf-3.17.3:\n",
            "      Successfully uninstalled protobuf-3.17.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.8.0 requires tf-estimator-nightly==2.8.0.dev2021122109, which is not installed.\n",
            "tensorflow 2.8.0 requires protobuf>=3.9.2, but you have protobuf 3.7.1 which is incompatible.\n",
            "tensorflow-metadata 1.7.0 requires protobuf<4,>=3.13, but you have protobuf 3.7.1 which is incompatible.\n",
            "tensorflow-hub 0.12.0 requires protobuf>=3.8.0, but you have protobuf 3.7.1 which is incompatible.\n",
            "pymc3 3.11.4 requires typing-extensions>=3.7.4, but you have typing-extensions 3.6.6 which is incompatible.\n",
            "googleapis-common-protos 1.55.0 requires protobuf>=3.12.0, but you have protobuf 3.7.1 which is incompatible.\n",
            "google-api-core 1.26.3 requires protobuf>=3.12.0, but you have protobuf 3.7.1 which is incompatible.\n",
            "bokeh 2.3.3 requires typing-extensions>=3.7.4, but you have typing-extensions 3.6.6 which is incompatible.\n",
            "arviz 0.11.4 requires typing-extensions<4,>=3.7.4.3, but you have typing-extensions 3.6.6 which is incompatible.\u001b[0m\n",
            "Successfully installed chainer-6.3.0 protobuf-3.7.1 typing-3.6.6 typing-extensions-3.6.6\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google",
                  "typing"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "  model.py\n",
        "'''\n",
        "import chainer\n",
        "import chainer.functions as F\n",
        "import chainer.links as L\n",
        "from chainer import cuda\n",
        "\n",
        "\n",
        "def init_conv(array):\n",
        "    xp = cuda.get_array_module(array)\n",
        "    array[...] = xp.random.normal(loc=0.0, scale=0.02, size=array.shape)\n",
        "\n",
        "\n",
        "def init_bn(array):\n",
        "    xp = cuda.get_array_module(array)\n",
        "    array[...] = xp.random.normal(loc=1.0, scale=0.02, size=array.shape)\n",
        "\n",
        "\n",
        "class ReLU(chainer.Chain):\n",
        "    def __init__(self):\n",
        "        super(ReLU, self).__init__()\n",
        "\n",
        "    def __call__(self, x):\n",
        "        return F.relu(x)\n",
        "\n",
        "\n",
        "class Tanh(chainer.Chain):\n",
        "    def __init__(self):\n",
        "        super(Tanh, self).__init__()\n",
        "\n",
        "    def __call__(self, x):\n",
        "        return F.tanh(x)\n",
        "\n",
        "\n",
        "class LeakyReLU(chainer.Chain):\n",
        "    def __init__(self):\n",
        "        super(LeakyReLU, self).__init__()\n",
        "\n",
        "    def __call__(self, x):\n",
        "        return F.leaky_relu(x)\n",
        "\n",
        "\n",
        "class DCGAN_G(chainer.ChainList):\n",
        "    def __init__(self, isize, nc, ngf, conv_init=None, bn_init=None):\n",
        "        cngf, tisize = ngf // 2, 4\n",
        "        while tisize != isize:\n",
        "            cngf = cngf * 2\n",
        "            tisize = tisize * 2\n",
        "\n",
        "        layers = []\n",
        "        # input is Z, going into a convolution\n",
        "        layers.append(L.Deconvolution2D(None, cngf, ksize=4, stride=1, pad=0, initialW=conv_init, nobias=True))\n",
        "        layers.append(L.BatchNormalization(cngf, initial_gamma=bn_init))\n",
        "        layers.append(ReLU())\n",
        "        csize, cndf = 4, cngf\n",
        "        while csize < isize // 2:\n",
        "            layers.append(L.Deconvolution2D(None, cngf // 2, ksize=4, stride=2, pad=1, initialW=conv_init, nobias=True))\n",
        "            layers.append(L.BatchNormalization(cngf // 2, initial_gamma=bn_init))\n",
        "            layers.append(ReLU())\n",
        "            cngf = cngf // 2\n",
        "            csize = csize * 2\n",
        "        layers.append(L.Deconvolution2D(None, nc, ksize=4, stride=2, pad=1, initialW=conv_init, nobias=True))\n",
        "        layers.append(Tanh())\n",
        "\n",
        "        super(DCGAN_G, self).__init__(*layers)\n",
        "\n",
        "    def __call__(self, x):\n",
        "        for i in range(len(self)):\n",
        "            x = self[i](x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class DCGAN_D(chainer.ChainList):\n",
        "    def __init__(self, isize, ndf, nz=1, conv_init=None, bn_init=None):\n",
        "        layers = []\n",
        "        layers.append(L.Convolution2D(None, ndf, ksize=4, stride=2, pad=1, initialW=conv_init, nobias=True))\n",
        "        layers.append(LeakyReLU())\n",
        "        csize, cndf = isize / 2, ndf\n",
        "        while csize > 4:\n",
        "            in_feat = cndf\n",
        "            out_feat = cndf * 2\n",
        "            layers.append(L.Convolution2D(None, out_feat, ksize=4, stride=2, pad=1, initialW=conv_init, nobias=True))\n",
        "            layers.append(L.BatchNormalization(out_feat, initial_gamma=bn_init))\n",
        "            layers.append(LeakyReLU())\n",
        "\n",
        "            cndf = cndf * 2\n",
        "            csize = csize / 2\n",
        "        # state size. K x 4 x 4\n",
        "        layers.append(L.Convolution2D(None, nz, ksize=4, stride=1, pad=0, initialW=conv_init, nobias=True))\n",
        "\n",
        "        super(DCGAN_D, self).__init__(*layers)\n",
        "\n",
        "    def encode(self, x):\n",
        "        for i in range(len(self)):\n",
        "            x = self[i](x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def __call__(self, x):\n",
        "        x = self.encode(x)\n",
        "        x = F.sum(x, axis=0) / x.shape[0]\n",
        "        return F.squeeze(x)\n",
        "\n",
        "\n",
        "class EncoderDecoder(chainer.Chain):\n",
        "    def __init__(self, nef, ngf, nc, nBottleneck, image_size=64, conv_init=None, bn_init=None):\n",
        "        super(EncoderDecoder, self).__init__(\n",
        "            encoder=DCGAN_D(image_size, nef, nBottleneck, conv_init, bn_init),\n",
        "            bn=L.BatchNormalization(nBottleneck, initial_gamma=bn_init),\n",
        "            decoder=DCGAN_G(image_size, nc, ngf, conv_init, bn_init)\n",
        "        )\n",
        "\n",
        "    def encode(self, x):\n",
        "        h = self.encoder.encode(x)\n",
        "        h = F.leaky_relu(self.bn(h))\n",
        "\n",
        "        return h\n",
        "\n",
        "    def decode(self, x):\n",
        "        h = self.decoder(x)\n",
        "\n",
        "        return h\n",
        "\n",
        "    def __call__(self, x):\n",
        "        h = self.encode(x)\n",
        "        h = self.decode(h)\n",
        "        return h"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yBgZUdC54tXX",
        "outputId": "5cb2c294-02e0-4371-a003-a96871e6bc4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/chainer/backends/cuda.py:143: UserWarning: cuDNN is not enabled.\n",
            "Please reinstall CuPy after you install cudnn\n",
            "(see https://docs-cupy.chainer.org/en/stable/install.html#install-cudnn).\n",
            "  'cuDNN is not enabled.\\n'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "  gp_gan.py\n",
        "'''\n",
        "import math\n",
        "\n",
        "import chainer\n",
        "import chainer.functions as F\n",
        "import numpy as np\n",
        "from chainer import cuda, Variable\n",
        "from scipy.fftpack import dct, idct\n",
        "from scipy.ndimage import correlate\n",
        "from scipy.optimize import minimize\n",
        "from skimage.filters import gaussian, sobel_h, sobel_v, scharr_h, scharr_v, roberts_pos_diag, roberts_neg_diag, \\\n",
        "    prewitt_h, prewitt_v\n",
        "from skimage.transform import resize\n",
        "\n",
        "################## Gradient Operator #########################\n",
        "normal_h = lambda im: correlate(im, np.asarray([[0, -1, 1]]), mode='nearest')\n",
        "normal_v = lambda im: correlate(im, np.asarray([[0, -1, 1]]).T, mode='nearest')\n",
        "\n",
        "gradient_operator = {\n",
        "    'normal': (normal_h, normal_v),\n",
        "    'sobel': (sobel_h, sobel_v),\n",
        "    'scharr': (scharr_h, scharr_v),\n",
        "    'roberts': (roberts_pos_diag, roberts_neg_diag),\n",
        "    'prewitt': (prewitt_h, prewitt_v)\n",
        "}\n",
        "\n",
        "\n",
        "###########################################################\n",
        "\n",
        "\n",
        "def preprocess(im):\n",
        "    im = np.transpose(im * 2 - 1, (2, 0, 1)).astype(np.float32)\n",
        "    return im\n",
        "\n",
        "\n",
        "def ndarray_resize(im, image_size, order=3, dtype=None):\n",
        "    im = resize(im, image_size, preserve_range=True, order=order, mode='constant')\n",
        "\n",
        "    if dtype:\n",
        "        im = im.astype(dtype)\n",
        "    return im\n",
        "\n",
        "\n",
        "def z_generate(z, G, copy_paste_var, nz, gpu):\n",
        "    z = np.reshape(z, (nz, 1, 1)).astype(np.float32)\n",
        "    z_var = Variable(chainer.dataset.concat_examples([z], gpu))\n",
        "\n",
        "    loss = F.mean_squared_error(copy_paste_var, G(z_var))\n",
        "\n",
        "    # Backward\n",
        "    loss.backward()\n",
        "    # Transfer loss & diff from GPU to CPU\n",
        "    loss = cuda.to_cpu(loss.data)\n",
        "    dz = np.squeeze(cuda.to_cpu(z_var.grad))\n",
        "\n",
        "    return loss, np.asarray(dz.flatten(), dtype=np.float64)\n",
        "\n",
        "\n",
        "def imfilter2d(im, filter_func):\n",
        "    gradients = np.zeros_like(im)\n",
        "    for i in range(im.shape[2]):\n",
        "        gradients[:, :, i] = filter_func(im[:, :, i])\n",
        "\n",
        "    return gradients\n",
        "\n",
        "\n",
        "def gradient_feature(im, color_feature, gradient_kernel):\n",
        "    result = np.zeros((*im.shape, 5))\n",
        "\n",
        "    gradient_h, gradient_v = gradient_operator[gradient_kernel]\n",
        "\n",
        "    result[:, :, :, 0] = color_feature\n",
        "    result[:, :, :, 1] = imfilter2d(im, gradient_h)\n",
        "    result[:, :, :, 2] = imfilter2d(im, gradient_v)\n",
        "    result[:, :, :, 3] = np.roll(result[:, :, :, 1], 1, axis=1)\n",
        "    result[:, :, :, 4] = np.roll(result[:, :, :, 2], 1, axis=0)\n",
        "\n",
        "    return result.astype(im.dtype)\n",
        "\n",
        "\n",
        "def fft2(K, size, dtype):\n",
        "    w, h = size\n",
        "    param = np.fft.fft2(K)\n",
        "    param = np.real(param[0:w, 0:h])\n",
        "\n",
        "    return param.astype(dtype)\n",
        "\n",
        "\n",
        "def laplacian_param(size, dtype):\n",
        "    w, h = size\n",
        "    K = np.zeros((2 * w, 2 * h)).astype(dtype)\n",
        "\n",
        "    laplacian_k = np.array([[0, -1, 0], [-1, 4, -1], [0, -1, 0]])\n",
        "    kw, kh = laplacian_k.shape\n",
        "    K[:kw, :kh] = laplacian_k\n",
        "\n",
        "    K = np.roll(K, -(kw // 2), axis=0)\n",
        "    K = np.roll(K, -(kh // 2), axis=1)\n",
        "\n",
        "    return fft2(K, size, dtype)\n",
        "\n",
        "\n",
        "def gaussian_param(size, dtype, sigma):\n",
        "    w, h = size\n",
        "    K = np.zeros((2 * w, 2 * h)).astype(dtype)\n",
        "\n",
        "    K[1, 1] = 1\n",
        "    K[:3, :3] = gaussian(K[:3, :3], sigma)\n",
        "\n",
        "    K = np.roll(K, -1, axis=0)\n",
        "    K = np.roll(K, -1, axis=1)\n",
        "\n",
        "    return fft2(K, size, dtype)\n",
        "\n",
        "\n",
        "def dct2(x, norm='ortho'):\n",
        "    return dct(dct(x, norm=norm).T, norm=norm).T\n",
        "\n",
        "\n",
        "def idct2(x, norm='ortho'):\n",
        "    return idct(idct(x, norm=norm).T, norm=norm).T\n",
        "\n",
        "\n",
        "def gaussian_poisson_editing(X, param_l, param_g, color_weight=1, eps=1e-12):\n",
        "    Fh = (X[:, :, :, 1] + np.roll(X[:, :, :, 3], -1, axis=1)) / 2\n",
        "    Fv = (X[:, :, :, 2] + np.roll(X[:, :, :, 4], -1, axis=0)) / 2\n",
        "    L = np.roll(Fh, 1, axis=1) + np.roll(Fv, 1, axis=0) - Fh - Fv\n",
        "\n",
        "    param = param_l + color_weight * param_g\n",
        "    param[(param >= 0) & (param < eps)] = eps\n",
        "    param[(param < 0) & (param > -eps)] = -eps\n",
        "\n",
        "    Y = np.zeros(X.shape[:3])\n",
        "    for i in range(3):\n",
        "        Xdct = dct2(X[:, :, i, 0])\n",
        "        Ydct = (dct2(L[:, :, i]) + color_weight * Xdct) / param\n",
        "        Y[:, :, i] = idct2(Ydct)\n",
        "    return Y\n",
        "\n",
        "\n",
        "def run_gp_editing(src_im, dst_im, mask_im, gan_im, color_weight, sigma, gradient_kernel='normal'):\n",
        "    dst_feature = gradient_feature(dst_im, gan_im, gradient_kernel)\n",
        "    src_feature = gradient_feature(src_im, gan_im, gradient_kernel)\n",
        "    feature = dst_feature * (1 - mask_im) + src_feature * mask_im\n",
        "\n",
        "    size, dtype = feature.shape[:2], feature.dtype\n",
        "    param_l = laplacian_param(size, dtype)\n",
        "    param_g = gaussian_param(size, dtype, sigma)\n",
        "    gan_im = gaussian_poisson_editing(feature, param_l, param_g, color_weight=color_weight)\n",
        "    gan_im = np.clip(gan_im, 0, 1)\n",
        "\n",
        "    return gan_im\n",
        "\n",
        "\n",
        "def laplacian_pyramid(im, max_level, image_size, smooth_sigma):\n",
        "    im_pyramid = [im]\n",
        "    diff_pyramid = []\n",
        "    for i in range(max_level - 1, -1, -1):\n",
        "        smoothed = gaussian(im_pyramid[-1], smooth_sigma, multichannel=True)\n",
        "        diff_pyramid.append(im_pyramid[-1] - smoothed)\n",
        "        smoothed = ndarray_resize(smoothed, (image_size * 2 ** i, image_size * 2 ** i))\n",
        "        im_pyramid.append(smoothed)\n",
        "\n",
        "    im_pyramid.reverse()\n",
        "    diff_pyramid.reverse()\n",
        "\n",
        "    return im_pyramid, diff_pyramid\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "GP-GAN: Towards Realistic High-Resolution Image Blending\n",
        "    obj:  source image,      size: w x h x 3, dtype: float, value: [0, 1]\n",
        "    bg :  destination image, size: w x h x 3, dtype: float, value: [0, 1]\n",
        "    mask: mask image,        size: w x h,     dtype: float, value: {0, 1}\n",
        "    G: Generator\n",
        "    image_size: image_size for Blending GAN\n",
        "    gpu: gpu id\n",
        "    color_weight: beta in Gaussion-Poisson Equation\n",
        "    sigma: sigma for gaussian smooth of Gaussian-Poisson Equation\n",
        "    gradient_kernel: kernel type for calc gradient\n",
        "    smooth_sigma: sigma for gaussian smooth of Laplacian pyramid\n",
        "    supervised: supervised Blending GAN ?\n",
        "    ## If supervised = False\n",
        "    nz: noise vector lendth\n",
        "    n_iteration: # of iterations for optimization\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def gp_gan(obj, bg, mask, G, image_size, gpu, color_weight=1, sigma=0.5, gradient_kernel='normal', smooth_sigma=1,\n",
        "           supervised=True, nz=100, n_iteration=1000):\n",
        "    w_orig, h_orig, _ = obj.shape\n",
        "    ############################ Gaussian-Poisson GAN Image Editing ###########################\n",
        "    # pyramid\n",
        "    max_level = int(math.ceil(np.log2(max(w_orig, h_orig) / image_size)))\n",
        "    obj_im_pyramid, _ = laplacian_pyramid(obj, max_level, image_size, smooth_sigma)\n",
        "    bg_im_pyramid, _ = laplacian_pyramid(bg, max_level, image_size, smooth_sigma)\n",
        "\n",
        "    # init GAN image\n",
        "    mask_init = ndarray_resize(mask, (image_size, image_size), order=0)[:, :, np.newaxis]\n",
        "    copy_paste_init = obj_im_pyramid[0] * mask_init + bg_im_pyramid[0] * (1 - mask_init)\n",
        "    copy_paste_init_var = Variable(chainer.dataset.concat_examples([preprocess(copy_paste_init)], gpu))\n",
        "\n",
        "    if supervised:\n",
        "        gan_im_var = G(copy_paste_init_var)\n",
        "    else:\n",
        "        z_init = np.random.normal(size=(nz, 1, 1))\n",
        "        res = minimize(z_generate, z_init, args=(G, copy_paste_init_var, nz, gpu), method='L-BFGS-B', jac=True,\n",
        "                       options={'maxiter': n_iteration, 'disp': False})\n",
        "        z = np.reshape(res.x, (nz, 1, 1)).astype(np.float32)\n",
        "        gan_im_var = G(Variable(chainer.dataset.concat_examples([z], gpu)))\n",
        "    gan_im = np.clip(np.transpose((np.squeeze(cuda.to_cpu(gan_im_var.data)) + 1) / 2, (1, 2, 0)), 0, 1).astype(\n",
        "        obj.dtype)\n",
        "\n",
        "    # Start pyramid\n",
        "    for level in range(max_level + 1):\n",
        "        size = obj_im_pyramid[level].shape[:2]\n",
        "        mask_im = ndarray_resize(mask, size, order=0)[:, :, np.newaxis, np.newaxis]\n",
        "        if level != 0:\n",
        "            gan_im = ndarray_resize(gan_im, size)\n",
        "\n",
        "        gan_im = run_gp_editing(obj_im_pyramid[level], bg_im_pyramid[level], mask_im, gan_im, color_weight, sigma,\n",
        "                                gradient_kernel)\n",
        "\n",
        "    gan_im = np.clip(gan_im * 255, 0, 255).astype(np.uint8)\n",
        "\n",
        "    return gan_im"
      ],
      "metadata": {
        "id": "Elldx3U04zK7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "  Reference: gp_gan_runner.py --> called by perform_blend.py\n",
        "  Calls gp_gan.py and model.py\n",
        "'''\n",
        "import argparse\n",
        "import os\n",
        "\n",
        "import chainer\n",
        "from chainer import cuda, serializers\n",
        "from skimage import img_as_float\n",
        "from skimage.io import imread, imsave\n",
        "\n",
        "basename = lambda path: os.path.splitext(os.path.basename(path))[0]\n",
        "\n",
        "\"\"\"\n",
        "    Note: source image, destination image and mask image have the same size.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def runner(sourceIMG, destinIMG, maskIMG, blend_id, supervised=False):\n",
        "    # parser = argparse.ArgumentParser(description='Gaussian-Poisson GAN for high-resolution image blending')\n",
        "    # parser.add_argument('--nef', type=int, default=64, help='# of base filters in encoder')\n",
        "    nef = 64\n",
        "    # parser.add_argument('--ngf', type=int, default=64, help='# of base filters in decoder or G')\n",
        "    ngf = 64\n",
        "    # parser.add_argument('--nc', type=int, default=3, help='# of output channels in decoder or G')\n",
        "    nc = 3\n",
        "    # parser.add_argument('--nBottleneck', type=int, default=4000, help='# of output channels in encoder')\n",
        "    nBottleneck = 4000\n",
        "    # parser.add_argument('--ndf', type=int, default=64, help='# of base filters in D')\n",
        "    ndf = 64\n",
        "    # parser.add_argument('--image_size', type=int, default=64, help='The height / width of the input image to network')\n",
        "    image_size = 64\n",
        "    # parser.add_argument('--color_weight', type=float, default=1, help='Color weight')\n",
        "    color_weight = 1\n",
        "    # parser.add_argument('--sigma', type=float, default=0.5,\n",
        "    #                     help='Sigma for gaussian smooth of Gaussian-Poisson Equation')\n",
        "    sigma = 0.5\n",
        "    # parser.add_argument('--gradient_kernel', type=str, default='normal', help='Kernel type for calc gradient')\n",
        "    gradient_kernel = 'normal'\n",
        "    # parser.add_argument('--smooth_sigma', type=float, default=1, help='Sigma for gaussian smooth of Laplacian pyramid')\n",
        "    smooth_sigma = 1\n",
        "    # parser.add_argument('--supervised', type=lambda x: x == 'True', default=True,\n",
        "    #                     help='Use unsupervised Blending GAN if False')\n",
        "    supervised = True\n",
        "    # parser.add_argument('--nz', type=int, default=100, help='Size of the latent z vector')\n",
        "    nz = 100\n",
        "    # parser.add_argument('--n_iteration', type=int, default=1000, help='# of iterations for optimizing z')\n",
        "    n_iteration = 10000\n",
        "    # parser.add_argument('--gpu', type=int, default=0, help='GPU ID (negative value indicates CPU)')\n",
        "    gpu = 0\n",
        "    # parser.add_argument('--g_path', default='models/blending_gan.npz', help='Path for pretrained Blending GAN model')\n",
        "    g_path = '/content/drive/My Drive/Final Project CS 329S/Blending GAN/Blending Sandbox Output/models/blending_gan.npz'\n",
        "    # parser.add_argument('--unsupervised_path', default='models/unsupervised_blending_gan.npz',\n",
        "    #                     help='Path for pretrained unsupervised Blending GAN model')\n",
        "    unsupervised_path='/content/drive/My Drive/Final Project CS 329S/Blending GAN/Blending Sandbox Output/models/unsupervised_blending_gan.npz'\n",
        "    # parser.add_argument('--list_path', default='',\n",
        "    #                     help='File for input list in csv format: obj_path;bg_path;mask_path in each line')\n",
        "    list_path =''\n",
        "    # parser.add_argument('--result_folder', default='blending_result', help='Name for folder storing results')\n",
        "    result_folder = 'blending_result'\n",
        "    # parser.add_argument('--src_image', default='', help='Path for source image')\n",
        "    src_image = sourceIMG\n",
        "    # parser.add_argument('--dst_image', default='', help='Path for destination image')\n",
        "    dst_image = destinIMG\n",
        "    # parser.add_argument('--mask_image', default='', help='Path for mask image')\n",
        "    mask_image = maskIMG\n",
        "    # parser.add_argument('--blended_image', default='', help='Where to save blended image')\n",
        "    blended_image = blend_id\n",
        "    # args = parser.parse_args()\n",
        "\n",
        "    # print('Input arguments:')\n",
        "    # for key, value in vars(args).items():\n",
        "    #     print('\\t{}: {}'.format(key, value))\n",
        "    # print('')\n",
        "\n",
        "    # Init CNN model\n",
        "    if supervised:\n",
        "        G = EncoderDecoder(nef, ngf, nc, nBottleneck, image_size=image_size)\n",
        "        print('Load pretrained Blending GAN model from {} ...'.format(g_path))\n",
        "        serializers.load_npz(g_path, G)\n",
        "    else:\n",
        "        chainer.config.use_cudnn = 'never'\n",
        "        G = DCGAN_G(image_size, nc, ngf)\n",
        "        print('Load pretrained unsupervised Blending GAN model from {} ...'.format(unsupervised_path))\n",
        "        serializers.load_npz(unsupervised_path, G)\n",
        "\n",
        "    if gpu >= 0:\n",
        "        cuda.get_device(gpu).use()  # Make a specified GPU current\n",
        "        G.to_gpu()  # Copy the model to the GPU\n",
        "\n",
        "    # Init image list\n",
        "    if list_path:\n",
        "        print('Load images from {} ...'.format(list_path))\n",
        "        with open(list_path) as f:\n",
        "            test_list = [line.strip().split(';') for line in f]\n",
        "        print('\\t {} images in total ...\\n'.format(len(test_list)))\n",
        "    else:\n",
        "        test_list = [(src_image, dst_image, mask_image)]\n",
        "\n",
        "    if not blended_image:\n",
        "        # Init result folder\n",
        "        if not os.path.isdir(result_folder):\n",
        "            os.makedirs(result_folder)\n",
        "        print('Result will save to {} ...\\n'.format(result_folder))\n",
        "\n",
        "    total_size = len(test_list)\n",
        "    for idx in range(total_size):\n",
        "        print('Processing {}/{} ...'.format(idx + 1, total_size))\n",
        "\n",
        "        # load image\n",
        "        obj = img_as_float(imread(test_list[idx][0]))\n",
        "        bg = img_as_float(imread(test_list[idx][1]))\n",
        "        mask = imread(test_list[idx][2], as_gray=True).astype(obj.dtype)\n",
        "\n",
        "        with chainer.using_config(\"train\", False):\n",
        "            blended_im = gp_gan(obj, bg, mask, G, image_size, gpu, color_weight=color_weight,\n",
        "                                sigma=sigma,\n",
        "                                gradient_kernel=gradient_kernel, smooth_sigma=smooth_sigma,\n",
        "                                supervised=supervised,\n",
        "                                nz=nz, n_iteration=n_iteration)\n",
        "\n",
        "        if blended_image:\n",
        "            imsave(blended_image, blended_im)\n",
        "        else:\n",
        "            imsave('{}/obj_{}_bg_{}_mask_{}.png'.format(result_folder, basename(test_list[idx][0]),\n",
        "                                                        basename(test_list[idx][1]), basename(test_list[idx][2])),\n",
        "                   blended_im)"
      ],
      "metadata": {
        "id": "yUTfs9qBywoS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "  reference: perform_blend.py\n",
        "  Class Definition of Merger\n",
        "  main.py calls get_res_blob(self, src1_prefix, src2_prefix, blend_blob_name)\n",
        "'''\n",
        "\n",
        "import os\n",
        "from google.cloud import storage\n",
        "import datetime\n",
        "\n",
        "\n",
        "class Merger:\n",
        "    \n",
        "    def list_buckets():\n",
        "        \"\"\"Lists all buckets.\"\"\"\n",
        "\n",
        "        storage_client = storage.Client()\n",
        "        buckets = storage_client.list_buckets()\n",
        "\n",
        "        for bucket in buckets:\n",
        "            print(bucket.name)\n",
        "    \n",
        "    def generate_download_signed_url_v4(bucket_name, blob_name):\n",
        "        \"\"\"Generates a v4 signed URL for downloading a blob.\n",
        "        Note that this method requires a service account key file. You can not use\n",
        "        this if you are using Application Default Credentials from Google Compute\n",
        "        Engine or from the Google Cloud SDK.\n",
        "        \"\"\"\n",
        "        # bucket_name = 'your-bucket-name'\n",
        "        # blob_name = 'your-object-name'\n",
        "\n",
        "        storage_client = storage.Client()\n",
        "        bucket = storage_client.bucket(bucket_name)\n",
        "        #print(bucket)\n",
        "        blob = bucket.blob(blob_name)\n",
        "        #print(bucket.list_blobs())\n",
        "        # for sub_folder in bucket.list_blobs():\n",
        "        #   print(sub_folder)\n",
        "        url = blob.generate_signed_url(\n",
        "            version=\"v4\",\n",
        "            # This URL is valid for 15 minutes\n",
        "            expiration=datetime.timedelta(minutes=15),\n",
        "            # Allow GET requests using this URL.\n",
        "            method=\"GET\",\n",
        "        )\n",
        "        # print(\"You can use this URL with any user agent, for example:\")\n",
        "        # print(\"curl '{}'\".format(url))\n",
        "        return url\n",
        "    \n",
        "    def upload_blob(bucket_name, source_file_name, destination_blob_name):\n",
        "        \"\"\"Uploads a file to the bucket.\"\"\"\n",
        "        # The ID of your GCS bucket\n",
        "        # bucket_name = \"your-bucket-name\"\n",
        "        # The path to your file to upload\n",
        "        # source_file_name = \"local/path/to/file\"\n",
        "        # The ID of your GCS object\n",
        "        # destination_blob_name = \"storage-object-name\"\n",
        "\n",
        "        storage_client = storage.Client()\n",
        "        bucket = storage_client.bucket(bucket_name)\n",
        "        blob = bucket.blob(destination_blob_name)\n",
        "\n",
        "        blob.upload_from_filename(source_file_name)\n",
        "\n",
        "        print(\n",
        "            \"File {} uploaded to {}.\".format(\n",
        "                source_file_name, destination_blob_name\n",
        "            )\n",
        "        )\n",
        "\n",
        "    def download_blob(bucket_name, source_blob_name, destination_file_name):\n",
        "        \"\"\"Downloads a blob from the bucket.\"\"\"\n",
        "        storage_client = storage.Client()\n",
        "        try:\n",
        "            bucket = storage_client.bucket(bucket_name)\n",
        "            blob = bucket.blob(source_blob_name)\n",
        "            blob.download_to_filename(destination_file_name)\n",
        "            print('file: ',destination_file_name,' downloaded from bucket: ',bucket_name,' successfully')\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "\n",
        "    def get(self):\n",
        "        bucket_name = os.environ.get(\"imagesearch-cs329s\",\n",
        "                                    app_identity.get_default_gcs_bucket_name())\n",
        "\n",
        "        self.response.headers['Content-Type'] = 'text/plain'\n",
        "        self.response.write('Demo GCS Application running from Version: '\n",
        "                            + os.environ['CURRENT_VERSION_ID'] + '\\n')\n",
        "        self.response.write('Using bucket name: ' + bucket_name + '\\n\\n')\n",
        "\n",
        "    # def get_res_url(self, source_url, destin_url, blend_id):\n",
        "    #     src_data = requests.get(source_url).content\n",
        "    #     destin_data = requests.get(destin_url).content\n",
        "       \n",
        "    #     # with open(os.path.join(os.getcwd(),'tocrop', 'src.jpg'), 'wb') as handler:\n",
        "    #     #     handler.write(src_data)\n",
        "    #     #     #handler.close()\n",
        "\n",
        "    #     #os.replace('src.jpg', os.path.join(os.getcwd(),'tocrop', 'src.jpg'))\n",
        "       \n",
        "        \n",
        "\n",
        "    #     # with open(os.path.join(os.getcwd(),'tocrop', 'dst.jpg'), 'wb') as handler:\n",
        "    #     #     handler.write(destin_data)\n",
        "    #     #     #handler.close()\n",
        "    #     #os.replace('dst.jpg', os.path.join(os.getcwd(),'tocrop', 'dst.jpg'))\n",
        "    #     # mask.jpg is local\n",
        "    #     mask = os.path.join(os.getcwd(),'tocrop', 'mask_display.png')\n",
        "        \n",
        "    #     cropper(source_url,destin_url)\n",
        "    #     # os.mkdir(os.path.join(os.getcwd(), 'tocrop'))\n",
        "    #     # os.mkdir(os.path.join(os.getcwd(), 'cropped'))\n",
        "    #     sourceIMG = os.path.join(os.getcwd(), 'cropped', 'crop_src.jpg')\n",
        "    #     destinIMG = os.path.join(os.getcwd(), 'cropped', 'crop_dst.jpg')\n",
        "    #     maskIMG = os.path.join(os.getcwd(), 'cropped', 'crop_mask.jpg')\n",
        "\n",
        "    #     runner(sourceIMG, destinIMG, maskIMG, blend_id, False)\n",
        "    #     blend_id_name = blend_id + '.jpg'\n",
        "    #     gan_result = os.path.join(os.getcwd(), blend_id_name)\n",
        "\n",
        "    #     upload_blob(\"imagesearch-cs329s\", gan_result , \"gan_result.jpg\")\n",
        "\n",
        "    #     return generate_download_signed_url_v4(\"imagesearch-cs329s\",\"gan_result.jpg\")\n",
        "\n",
        "    def get_first_blob(self, bucket_name, prefix, delimiter = None):\n",
        "        bucket_name = \"imagesearch-cs329s\"\n",
        "        storage_client = storage.Client()\n",
        "        blobs = storage_client.list_blobs(bucket_name, prefix = prefix)\n",
        "\n",
        "        if blobs:\n",
        "\n",
        "            count = 0\n",
        "            for blob in blobs:\n",
        "                if count == 1:\n",
        "                    filename = os.path.join('tocrop',str(blob.name).split('/')[1])\n",
        "                    blob.download_to_filename(filename)\n",
        "                    return filename\n",
        "                else:\n",
        "                    count = count + 1\n",
        "\n",
        "    def get_res_blob(self, src1_prefix, src2_prefix, blend_blob_name):\n",
        "        # src1_img_name = str(self.get_first_blob(\"imagesearch-cs329s\", src1_prefix).split('/')[1])\n",
        "        # src2_img_name = str(self.get_first_blob(\"imagesearch-cs329s\", src2_prefix).split('/')[1])\n",
        "        # src1_img = self.get_first_blob(\"imagesearch-cs329s\", src1_prefix)\n",
        "        # src2_img = self.get_first_blob(\"imagesearch-cs329s\", src2_prefix)\n",
        "        src1_img_name = src1_prefix + '.jpg'\n",
        "        src2_img_name = src2_prefix + '.jpg'\n",
        "\n",
        "        #os.replace('mask_display.png', os.path.join(os.getcwd(),'tocrop', 'mask.png'))\n",
        "        mask_name = 'mask_middle.png'\n",
        "        \n",
        "        cropper()\n",
        "        sourceIMG = os.path.join(cropped, 'crop_'+ src1_img_name )\n",
        "        destinIMG = os.path.join(cropped, 'crop_' + src2_img_name)\n",
        "        maskIMG = os.path.join(cropped, 'crop_' + mask_name)\n",
        "        runner(sourceIMG, destinIMG, maskIMG, blend_blob_name+'.jpg', False)\n",
        "        blend_id_name = blend_blob_name + '.jpg'\n",
        "        gan_result = os.path.join(os.getcwd(), blend_id_name)\n",
        "\n",
        "        #upload_blob(\"imagesearch-cs329s\", gan_result , \"blended_gan_result.jpg\")\n",
        "\n",
        "        #return generate_download_signed_url_v4(\"imagesearch-cs329s\",\"gan_result.jpg\")\n",
        "        return \"blended_gan_result.jpg\""
      ],
      "metadata": {
        "id": "48zKdLu6yCGB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "chdir = \"/content/drive/My Drive/Final Project CS 329S/Blending GAN/Blending Sandbox Output\"\n",
        "os.chdir(chdir)\n",
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0JCJaep65t7Z",
        "outputId": "e60bb614-e920-4406-9b06-0bec04bbb9b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: chainer==6.3.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 1)) (6.3.0)\n",
            "Collecting cupy==6.3.0\n",
            "  Downloading cupy-6.3.0.tar.gz (3.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1 MB 12.8 MB/s \n",
            "\u001b[?25hCollecting scikit-image==0.15.0\n",
            "  Downloading scikit_image-0.15.0-cp37-cp37m-manylinux1_x86_64.whl (26.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 26.3 MB 1.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from chainer==6.3.0->-r requirements.txt (line 1)) (1.15.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from chainer==6.3.0->-r requirements.txt (line 1)) (3.6.0)\n",
            "Requirement already satisfied: typing<=3.6.6 in /usr/local/lib/python3.7/dist-packages (from chainer==6.3.0->-r requirements.txt (line 1)) (3.6.6)\n",
            "Requirement already satisfied: protobuf<3.8.0rc1,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from chainer==6.3.0->-r requirements.txt (line 1)) (3.7.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from chainer==6.3.0->-r requirements.txt (line 1)) (57.4.0)\n",
            "Requirement already satisfied: typing-extensions<=3.6.6 in /usr/local/lib/python3.7/dist-packages (from chainer==6.3.0->-r requirements.txt (line 1)) (3.6.6)\n",
            "Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from chainer==6.3.0->-r requirements.txt (line 1)) (1.21.5)\n",
            "Requirement already satisfied: fastrlock>=0.3 in /usr/local/lib/python3.7/dist-packages (from cupy==6.3.0->-r requirements.txt (line 2)) (0.8)\n",
            "Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image==0.15.0->-r requirements.txt (line 3)) (1.2.0)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image==0.15.0->-r requirements.txt (line 3)) (2.6.3)\n",
            "Requirement already satisfied: imageio>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image==0.15.0->-r requirements.txt (line 3)) (2.4.1)\n",
            "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image==0.15.0->-r requirements.txt (line 3)) (3.2.2)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image==0.15.0->-r requirements.txt (line 3)) (1.4.1)\n",
            "Requirement already satisfied: pillow>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image==0.15.0->-r requirements.txt (line 3)) (7.1.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image==0.15.0->-r requirements.txt (line 3)) (1.3.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image==0.15.0->-r requirements.txt (line 3)) (3.0.7)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image==0.15.0->-r requirements.txt (line 3)) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image==0.15.0->-r requirements.txt (line 3)) (0.11.0)\n",
            "Building wheels for collected packages: cupy\n",
            "  Building wheel for cupy (setup.py) ... \u001b[?25lerror\n",
            "\u001b[31m  ERROR: Failed building wheel for cupy\u001b[0m\n",
            "\u001b[?25h  Running setup.py clean for cupy\n",
            "Failed to build cupy\n",
            "Installing collected packages: scikit-image, cupy\n",
            "  Attempting uninstall: scikit-image\n",
            "    Found existing installation: scikit-image 0.18.3\n",
            "    Uninstalling scikit-image-0.18.3:\n",
            "      Successfully uninstalled scikit-image-0.18.3\n",
            "    Running setup.py install for cupy ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[31mERROR: Command errored out with exit status 1: /usr/bin/python3 -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-install-7isuexoq/cupy_6c1872b3f893470ba5374ab71406897c/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-install-7isuexoq/cupy_6c1872b3f893470ba5374ab71406897c/setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record /tmp/pip-record-6btk22j3/install-record.txt --single-version-externally-managed --compile --install-headers /usr/local/include/python3.7/cupy Check the logs for full command output.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "blender = Merger()\n",
        "\n",
        "source = 'src_nature'\n",
        "destin = 'dst_nature'\n",
        "blended = 'blended_gan_result2'\n",
        "\n",
        "result_BLOB = blender.get_res_blob(source, destin, blended)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "79D48xit26cC",
        "outputId": "dbd658e3-91d4-4545-fb96-a167f94ba863"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Load pretrained unsupervised Blending GAN model from /content/drive/My Drive/Final Project CS 329S/Blending GAN/Blending Sandbox Output/models/unsupervised_blending_gan.npz ...\n",
            "Processing 1/1 ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4,5,476,494\n",
        "\n",
        "# crop high-res living room\n",
        "\n",
        "\n",
        "# Importing Image class from PIL module\n",
        "from PIL import Image\n",
        " \n",
        "# Opens a image in RGB mode\n",
        "im = Image.open(r\"/content/drive/My Drive/Final Project CS 329S/Blending GAN/Blending Sandbox Output/images/left_right_images/tocrop/blue-arm-chair.jpg\")\n",
        "os.chdir(\"/content/drive/My Drive/Final Project CS 329S/Blending GAN/Blending Sandbox Output/images/left_right_images/cropped\")\n",
        "\n",
        "# Size of the image in pixels (size of original image)\n",
        "# (This is not mandatory)\n",
        "width, height = im.size\n",
        "'''4,5,476,494'''\n",
        " \n",
        "# Setting the points for cropped image\n",
        "left = 1*width/7\n",
        "top = height / 5\n",
        "right = 6*width/7\n",
        "bottom = 4*height/5\n",
        " \n",
        "# Cropped image of above dimension\n",
        "# (It will not change original image)\n",
        "#im1 = im.crop((left, top, right, bottom))\n",
        "newsize = (472, 489)\n",
        "im1 = im.resize(newsize)\n",
        "\n",
        "im1.save(\"scaledblue-arm-chair.png\")"
      ],
      "metadata": {
        "id": "aqVtrdqOQBiu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from PIL import image\n",
        "\n",
        "# immask = Image.open(r\"/content/drive/My Drive/Final Project CS 329S/Blending GAN/Blending Sandbox Output/images/left_right_images/tocrop/scaledantique-coffee-table.jpg\")\n",
        "# mask_img = Image.open(immask)\n",
        "# datas_mask = mask_img.getdata()\n",
        "\n",
        "# newData = []\n",
        "# for item in range(len(datas_mask)):\n",
        "#     if datas_fore[item][0] == 255 and datas_fore[item][1] == 255 and datas_fore[item][2] == 255:\n",
        "#         newData.append((datas_back[item][0], datas_back[item][1], datas_back[item][2]))\n",
        "#     else:\n",
        "#         newData.append((datas_fore[item][0], datas_fore[item][1], datas_fore[item][2]))\n",
        "#     print(datas_fore[item][0])\n",
        "\n",
        "\n",
        "# mask_img.putdata(newData)\n",
        "# mask_img.save(\"mask_coffee.png\", \"PNG\")"
      ],
      "metadata": {
        "id": "hqpGR_OKsR8_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generate white mask\n",
        "from PIL import Image\n",
        "\n",
        "immask = Image.open(r\"/content/drive/My Drive/Final Project CS 329S/Blending GAN/Blending Sandbox Output/images/left_right_images/tocrop/scaledantique-coffee-table.jpg\")\n",
        "os.chdir(\"/content/drive/My Drive/Final Project CS 329S/Blending GAN/Blending Sandbox Output/images/left_right_images/tocrop\")\n",
        "\n",
        "datas_mask = immask.getdata()\n",
        "\n",
        "newData = []\n",
        "for item in range(len(datas_mask)):\n",
        "    newData.append((255,255,255))\n",
        "\n",
        "immask.putdata(newData)\n",
        "immask.save(\"mask_coffee.png\", \"PNG\")\n"
      ],
      "metadata": {
        "id": "rInrgXO6t84E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# padding coffee table\n",
        "from PIL import Image, ImageOps\n",
        "\n",
        "im_pth_coffee = \"/content/drive/My Drive/Final Project CS 329S/Blending GAN/Blending Sandbox Output/images/left_right_images/tocrop/scaledantique-coffee-table.jpg\"\n",
        "im_pth_blue = \"/content/drive/My Drive/Final Project CS 329S/Blending GAN/Blending Sandbox Output/images/left_right_images/cropped/scaledblue-arm-chair.png\"\n",
        "\n",
        "os.chdir(\"/content/drive/My Drive/Final Project CS 329S/Blending GAN/Blending Sandbox Output/images/left_right_images/cropped\")\n",
        "\n",
        "imc = Image.open(im_pth_coffee)\n",
        "imb = Image.open(im_pth_blue)\n",
        "#old_size = im.size  # old_size[0] is in (width, height) format\n",
        "\n",
        "# ratio = float(desired_size)/max(old_size)\n",
        "# new_size = tuple([int(x*ratio) for x in old_size])\n",
        "# use thumbnail() or resize() method to resize the input image\n",
        "\n",
        "# thumbnail is a in-place operation\n",
        "\n",
        "# im.thumbnail(new_size, Image.ANTIALIAS)\n",
        "\n",
        "imc = imc.resize((170,170))\n",
        "imb = imb.resize((230,240))\n",
        "# create a new image and paste the resized on it\n",
        "\n",
        "new_im = Image.new(\"RGB\", (472,489), (255,255,255)) #white background\n",
        "new_im.paste(imc, (0,280))\n",
        "new_im.paste(imb, (165,270))\n",
        "\n",
        "new_im.save('both_objects_white.jpg')"
      ],
      "metadata": {
        "id": "g4gaB5KS0NqR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def demo_runner(destinIMG_test):\n",
        "  from PIL import Image\n",
        "\n",
        "  os.chdir(\"/content/drive/My Drive/Final Project CS 329S/Blending GAN/Blending Sandbox Output/images/left_right_images/cropped\")\n",
        "\n",
        "  foreground = \"/content/drive/My Drive/Final Project CS 329S/Blending GAN/Blending Sandbox Output/images/left_right_images/cropped/both_objects_white.jpg\"\n",
        "  background = destinIMG_test\n",
        "  fore_img = Image.open(foreground)\n",
        "  back_img = Image.open(background)\n",
        "  #fore_img = fore_img.convert(\"RGBA\")\n",
        "  datas_fore = fore_img.getdata()\n",
        "  datas_back = back_img.getdata()\n",
        "\n",
        "  newData = []\n",
        "  for item in range(len(datas_fore)):\n",
        "      if datas_fore[item][0] + datas_fore[item][1] + datas_fore[item][2] > 700:\n",
        "          newData.append((datas_back[item][0], datas_back[item][1], datas_back[item][2]))\n",
        "      else:\n",
        "          newData.append((datas_fore[item][0], datas_fore[item][1], datas_fore[item][2]))\n",
        "      #print(datas_fore[item][0])\n",
        "\n",
        "\n",
        "  fore_img.putdata(newData)\n",
        "  fore_img.save(\"gan_result_maskwhite.jpg\")"
      ],
      "metadata": {
        "id": "kFfeCDHX7sMO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir(\"/content/drive/My Drive/Final Project CS 329S/Blending GAN/Blending Sandbox Output/images/left_right_images\")\n",
        "sourceIMG_test = \"/content/drive/My Drive/Final Project CS 329S/Blending GAN/Blending Sandbox Output/images/left_right_images/cropped/gan_result_maskwhite.jpg\"\n",
        "destinIMG_test = \"/content/drive/My Drive/Final Project CS 329S/Blending GAN/Blending Sandbox Output/images/left_right_images/tocrop/scaledbigliving.png\"\n",
        "maskIMG_test = \"/content/drive/My Drive/Final Project CS 329S/Blending GAN/Blending Sandbox Output/images/left_right_images/tocrop/mask_coffee.png\"\n",
        "\n",
        "demo_runner(destinIMG_test)\n",
        "\n",
        "blend_blob_name = \"demo_gan\"\n",
        "\n",
        "\n",
        "# Warning: using unsupervised (red-tint) model\n",
        "runner(sourceIMG_test, destinIMG_test, maskIMG_test, blend_blob_name+'.jpg', False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IoXF5EaKAWWp",
        "outputId": "16a05bfc-d596-45e4-d6ee-0d191e690246"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Load pretrained Blending GAN model from /content/drive/My Drive/Final Project CS 329S/Blending GAN/Blending Sandbox Output/models/blending_gan.npz ...\n",
            "Processing 1/1 ...\n"
          ]
        }
      ]
    }
  ]
}